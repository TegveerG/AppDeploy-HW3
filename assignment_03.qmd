---
title: Assignment 03
subtitle: Hyperparameter Tunning - Group Number 11
date: 09/29/2023
date-modified: last-modified
date-format: long
format:
  html:
    theme:
      - cosmo
      - theme.scss
    toc: true
    embed-resources: true
    number-sections: true
author:
  - name: Tegveer Ghura
    affiliations:
      - id: gu
        name: Georgetown University
        city: Washington
        state: DC
jupyter: python3
---

# Instructions (Remove the instructions before submission)

This assignment will deal with tuning the hyperparameters for the [online shopping dataset](https://archive.ics.uci.edu/dataset/468/online+shoppers+purchasing+intention+dataset). Make sure to remove the instructions and only keep Q6 onward. The qmd file of this assignment is located in the [files folder](https://georgetown.instructure.com/files/11681026/download?download_frd=1).

- This is a group assignment with independent submission on Canvas. Collaboration is essential. Use Git for version control.
- Begin by setting your random seed as the last four digits of your GUID.
- Prefix each variable with 'g#groupnumber' (e.g., g01_variableName) to ensure uniqueness and to demonstrate originality in your group's work.
- add the names of all group members to the YAML header above.
- Use of Generative AI tools, including but not restricted to GPT-3 is strictly prohibited.

## Git Commit and Collaboration

- This is a group assignment. Collaboration is essential. Use Git for version control.
- Regular and meaningful commit messages are expected, indicating steady progress and contributions from all group members.
- Avoid large, infrequent commits. Instead, aim for more minor, frequent updates showing your code's evolution and thoughts.
- Collaboration tools, especially Git, should be used as a backup tool and a truly collaborative platform. Discuss, review, and merge each other's contributions.

# Grading Criteria

- The assignment is worth 75 points.
- There are three grading milestones in the assignment.
  - Adherence to Requirements, Coding Standards, Documentation, Runtime, and Efficiency (22 Points)
    - Adherence to Requirements (5 Points): Ensure all the given requirements of the assignment, including Git commits and collaboration, are met.
    - Coding Standards (5 Points): Code should be readable and maintainable. Ensure appropriate variable naming and code commenting.
    - Documentation (6 Points): Provide explanations or reasoning for using a particular command and describe the outputs. Avoid vague descriptions; aim for clarity and depth.
    - Runtime (3 Points): The code should execute without errors and handle possible exceptions.
    - Efficiency (3 Points): Implement efficient coding practices, avoid redundancy, and optimize for performance where applicable.
  - Collaborative Programming (13 Points)
    - GitHub Repository Structure (3 Points): A well-organized repository with clear directory structures and meaningful file names.
    - Number of Commits (3 Points): Reflects steady progress and contributions from all group members.
    - Commit Quality (3 Points): Clear, descriptive commit messages representing logical chunks of work. Avoid trivial commits like "typo fix."
    - Collaboration & Contribution (4 Points): Demonstrated teamwork where each member contributes significantly. This can be seen through pull requests, code reviews, and merge activities.
  - Assignment Questions (40 Points)

# Adherence to Requirements, Coding Standards, Documentation, Runtime, and Efficiency (22 Points)
This section is graded based on adherence to Requirements, Coding Standards,
Documentation, Runtime, and Efficiency.

# Collaborative Programming (13 Points)

This section is graded based on the Github submission. Each person needs to have made commits to the repository. GitHub Repository Structure, Number of Commits, Commit Quality, Collaboration, and Contribution are generally graded based on the group's overall performance. However, if there is a significant difference in the number of commits or contributions between group members, the instructor may adjust the grade accordingly.


# Assignment Questions (40 Points)

# Data Preparation (7 Points):

## Import the necessary libraries

```{python}
import random
random.seed(1310) # last 4 GUID digits
from seaborn.palettes import color_palette
from seaborn import set_palette
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd
from sklearn.model_selection import cross_val_score, RepeatedKFold, GridSearchCV, RandomizedSearchCV
from sklearn.ensemble import BaggingClassifier, RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split
import pickle
import warnings
warnings.filterwarnings('ignore')
```

## Load the dataset and display the dataframe (2 Points).

```{python}
filename = '../HW2/creditcard.csv'
CC = pd.read_csv(filename)

CC.head(6)
```

## Use `describe` to provide statistics on the pandas Dataframe (2 Points).

```{python}
# Add code here
CC.describe()
```

## Convert To Correct Dtypes and Clean DataFrame

```{python}
#| colab: {base_uri: 'https://localhost:8080/'}
#| outputId: 8f115a25-886d-43aa-fb1f-8fea814c0ed2
# Class is categorical variables but is encoded as integer
CC['Class'] = CC['Class'].astype('category')

# Drop NA's if any

CC.dropna(inplace=True)
```

## Split the dataset into a Training set and a Test set. Justify your preferred split (3 Points).

```{python}
X_train, X_test, y_train, y_test = train_test_split(CC.iloc[:,0:-2],
                                        CC.iloc[:,-1],
                                        test_size=0.33,
                                        random_state=42)

# Reset and drop indexes after splitting, convert to numpy arrays and flatten
X_train.reset_index(inplace=True)
X_train = X_train.to_numpy()
y_train = y_train.reset_index().drop("index", axis=1).astype('category')
y_train = y_train.to_numpy().reshape(len(y_train),)

X_test.reset_index(inplace=True)
X_test = X_test.to_numpy()
y_test = y_test.reset_index().drop("index", axis=1).astype('category')
y_test = y_test.to_numpy().reshape(len(y_test),)
```

# Classification Routine (12 Points)

Execute a classification routine using RandomForestClassifier(), BaggingClassifier(), and XGboostclassifier(). Independently output the accuracy box plot as discussed in class. Use any package you are comfortable with (seaborn, matplotlib).

## RandomForestClassifier()

```{python}
%%time
RF = RandomForestClassifier()

cv = RepeatedKFold(n_splits=5, n_repeats=2, random_state=1)

RF_accuracy = cross_val_score(RF, X_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')

RF_weighted_f1 = cross_val_score(RF, X_train, y_train, scoring='f1_weighted', cv=cv, n_jobs=-1, error_score='raise')

mean_accuracy_percentage = RF_accuracy.mean() * 100
mean_weighted_f1_percentage = RF_weighted_f1.mean() * 100

print('Random Forest Mean Accuracy: %.3f, Mean Weighted F1: %.3f' % (mean_accuracy_percentage, mean_weighted_f1_percentage))
```

```{python}
results = {}
results['Accuracy'] = RF_accuracy
results['Weighted_F1'] = RF_weighted_f1

RF_mod = pd.DataFrame(results)

# Melt the DataFrame to have 'Metric' as a new column indicating Accuracy or Weighted_F1
RF_mod_melted = pd.melt(RF_mod, value_vars=['Accuracy', 'Weighted_F1'], var_name='Metric', value_name='Value')

# Plotting using Seaborn
plt.figure(figsize=(8, 6))
sns.boxplot(x='Metric', y='Value', data=RF_mod_melted)
plt.xlabel('Metric')
plt.ylabel('Value')
plt.title('Random Forest Training Set Accuracy vs Weighted_F1')
plt.show()
plt.savefig('products/plots/RF.jpeg')
```

```{python}
%%time

RF.fit(X_train, y_train)

# Save to file in the current working directory
pkl_filename = "products/pickle_files/RF.pkl"
with open(pkl_filename, 'wb') as file:
  pickle.dump(RF, file)

# Load from file
with open(pkl_filename, 'rb') as file:
  pickle_model = pickle.load(file)
  score = pickle_model.score(X_test, y_test)
  print("Random Forest Test Set Accuracy: {0:.2f} %".format(100 * score))
  Y_predict = pickle_model.predict(X_test)
```

```{python}
conf_matrix = confusion_matrix(y_test, Y_predict)
disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=['Not Fraud','Fraud'], )

# save the plot

disp.plot(cmap='YlGnBu',values_format='d',ax=None)
plt.title('Test Set Confusion Matrix for Random Forest', fontsize=15, color='#336699',loc='center')
plt.savefig('products/plots/confusion_matrix_RF.png')
```

## BaggingClassifier()

```{python}
%%time

Bagging = BaggingClassifier()

cv = RepeatedKFold(n_splits=5, n_repeats=2, random_state=1)

Bagging_accuracy = cross_val_score(Bagging, X_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')

Bagging_weighted_f1 = cross_val_score(Bagging, X_train, y_train, scoring='f1_weighted', cv=cv, n_jobs=-1, error_score='raise')

mean_accuracy_percentage = Bagging_accuracy.mean() * 100
mean_weighted_f1_percentage = Bagging_weighted_f1.mean() * 100

print('Bagging Mean Accuracy: %.3f, Mean Weighted F1: %.3f' % (mean_accuracy_percentage, mean_weighted_f1_percentage))
```

```{python}
results = {}
results['Accuracy'] = Bagging_accuracy
results['Weighted_F1'] = Bagging_weighted_f1

Bagging_mod = pd.DataFrame(results)

# Melt the DataFrame to have 'Metric' as a new column indicating Accuracy or Weighted_F1
Bagging_mod_melted = pd.melt(Bagging_mod, value_vars=['Accuracy', 'Weighted_F1'], var_name='Metric', value_name='Value')

# Plotting using Seaborn
plt.figure(figsize=(8, 6))
sns.boxplot(x='Metric', y='Value', data=Bagging_mod_melted)
plt.xlabel('Metric')
plt.ylabel('Value')
plt.title('Bagging Training Set Accuracy vs Weighted_F1')
plt.show()
plt.savefig('products/plots/Bagging.jpeg')
```

```{python}
%%time

Bagging.fit(X_train, y_train)

# Save to file in the current working directory
pkl_filename = "products/pickle_files/Bagging.pkl"
with open(pkl_filename, 'wb') as file:
  pickle.dump(Bagging, file)

# Load from file
with open(pkl_filename, 'rb') as file:
  pickle_model = pickle.load(file)
  score = pickle_model.score(X_test, y_test)
  print("Bagging Test Set Accuracy: {0:.2f} %".format(100 * score))
  Y_predict = pickle_model.predict(X_test)
```

```{python}
conf_matrix = confusion_matrix(y_test, Y_predict)
disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=['Not Fraud','Fraud'], )

# save the plot

disp.plot(cmap='YlGnBu',values_format='d',ax=None)
plt.title('Test Set Confusion Matrix for Bagging', fontsize=15, color='#336699',loc='center')
plt.savefig('products/plots/confusion_matrix_Bagging.png')
```

## XGboostclassifier()

```{python}
%%time

XGB = XGBClassifier()

cv = RepeatedKFold(n_splits=5, n_repeats=2, random_state=1)

XGB_accuracy = cross_val_score(XGB, X_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')

XGB_weighted_f1 = cross_val_score(XGB, X_train, y_train, scoring='f1_weighted', cv=cv, n_jobs=-1, error_score='raise')

mean_accuracy_percentage = XGB_accuracy.mean() * 100
mean_weighted_f1_percentage = XGB_weighted_f1.mean() * 100

print('XGBoost Mean Accuracy: %.3f, Mean Weighted F1: %.3f' % (mean_accuracy_percentage, mean_weighted_f1_percentage))
```

```{python}
results = {}
results['Accuracy'] = XGB_accuracy
results['Weighted_F1'] = XGB_weighted_f1

XGB_mod = pd.DataFrame(results)

# Melt the DataFrame to have 'Metric' as a new column indicating Accuracy or Weighted_F1
XGB_mod_melted = pd.melt(XGB_mod, value_vars=['Accuracy', 'Weighted_F1'], var_name='Metric', value_name='Value')

# Plotting using Seaborn
plt.figure(figsize=(8, 6))
sns.boxplot(x='Metric', y='Value', data=XGB_mod_melted)
plt.xlabel('Metric')
plt.ylabel('Value')
plt.title('XGBoost Training Set Accuracy vs Weighted_F1')
plt.show()
plt.savefig('products/plots/XGB.jpeg')
```

```{python}
%%time

XGB.fit(X_train, y_train)

# Save to file in the current working directory
pkl_filename = "products/pickle_files/XGB.pkl"
with open(pkl_filename, 'wb') as file:
  pickle.dump(XGB, file)

# Load from file
with open(pkl_filename, 'rb') as file:
  pickle_model = pickle.load(file)
  score = pickle_model.score(X_test, y_test)
  print("XGBoost Test Set Accuracy: {0:.2f} %".format(100 * score))
  Y_predict = pickle_model.predict(X_test)
```

```{python}
conf_matrix = confusion_matrix(y_test, Y_predict)
disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=['Not Fraud','Fraud'], )

# save the plot

disp.plot(cmap='YlGnBu',values_format='d',ax=None)
plt.title('Test Set Confusion Matrix for XGBoost', fontsize=15, color='#336699',loc='center')
plt.savefig('products/plots/confusion_matrix_XGB.png')
```

# Classification with GridSearchCV (8 Points)

## RandomForestClassifier()

```{python}
%%time

# defining parameter range
param_grid = params = {
  'n_estimators': [1, 2, 4, 8, 16, 32, 64, 100, 200],
  'max_depth': [3, 5, 7],
  'criterion': ["gini", "entropy"],
  'min_samples_leaf': [0.1, 0.2, 0.3, 0.4, 0.5],
  'max_features': list(range(1,X_train.shape[1])),
}
  
grid = GridSearchCV(RandomForestClassifier(), param_grid, refit = True, verbose = 0, scoring='accuracy', cv = 5, n_jobs=-1)

grid.fit(X_train, y_train)

# print best parameter after tuning
print("The best parameters after tuning are: ", grid.best_params_)
  
# print how our model looks after hyper-parameter tuning
print("The best model after tuning looks like: ",grid.best_estimator_)

# Save to file in the current working directory
pkl_filename = "products/pickle_files/RF_Grid_Tuned.pkl"
with open(pkl_filename, 'wb') as file:
  pickle.dump(grid, file)

# Load from file
with open(pkl_filename, 'rb') as file:
  pickle_model = pickle.load(file)
  score = pickle_model.score(X_test, y_test)
  print("Grid-Search Tuned Random Forest Test Set Accuracy: {0:.2f} %".format(100 * score))
  grid_predictions = pickle_model.predict(X_test)
```

```{python}
conf_matrix = confusion_matrix(y_test, grid_predictions)
disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=['Not Fraud','Fraud'], )

# save the plot

disp.plot(cmap='YlGnBu',values_format='d',ax=None)
plt.title('Test Set Confusion Matrix for Grid-Search Tuned Random Forest', fontsize=15, color='#336699',loc='center')
plt.savefig('products/plots/confusion_matrix_RF_Grid_Tuned.png')
```

## BaggingClassifier()

```{python}
%%time

# defining parameter range
param_grid = params = {
    'n_estimators': [1, 2, 4, 8, 16, 32, 64, 100, 200],
    'max_samples': [0.1*X_train.shape[0], 0.3*X_train.shape[0], 0.5*X_train.shape[0], 0.7*X_train.shape[0], X_train.shape[0]],
    'max_features': [0.1*X_train.shape[1], 0.3*X_train.shape[1], 0.5*X_train.shape[1], 0.7*X_train.shape[1], X_train.shape[1]],
    'bootstrap_features': [True, False],
    'bootstrap': [True, False],
    'oob_score': [True, False],
    'warm_start': [True, False],
}
  
grid = GridSearchCV(BaggingClassifier(), param_grid, refit = True, verbose = 0, scoring='accuracy', cv = 5, n_jobs=-1)

grid.fit(X_train, y_train)

# print best parameter after tuning
print("The best parameters after tuning are: ", grid.best_params_)
  
# print how our model looks after hyper-parameter tuning
print("The best model after tuning looks like: ",grid.best_estimator_)

# Save to file in the current working directory
pkl_filename = "products/pickle_files/Bagging_Grid_Tuned.pkl"
with open(pkl_filename, 'wb') as file:
  pickle.dump(grid, file)

# Load from file
with open(pkl_filename, 'rb') as file:
  pickle_model = pickle.load(file)
  score = pickle_model.score(X_test, y_test)
  print("Grid-Search Tuned Bagging Test Set Accuracy: {0:.2f} %".format(100 * score))
  grid_predictions = pickle_model.predict(X_test)
```

```{python}
conf_matrix = confusion_matrix(y_test, grid_predictions)
disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=['Not Fraud','Fraud'], )

# save the plot

disp.plot(cmap='YlGnBu',values_format='d',ax=None)
plt.title('Test Set Confusion Matrix for Grid-Search Tuned Bagging', fontsize=15, color='#336699',loc='center')
plt.savefig('products/plots/confusion_matrix_Bagging_Grid_Tuned.png')
```

## XGBClassifier()

```{python}
%%time

# defining parameter range
param_grid = params = {
    'max_depth': [2, 3, 5, 10, 20],
    'max_leaves': [0, 1, 2, 3, 4, 5],
    'learning_rate': [1, 0.1, 0.01, 0.001, 0.0001],
    'n_estimators': [1, 2, 4, 8, 16, 32, 64, 100, 200],
    'booster': ['gbtree', 'gblinear', 'dart'],
    'gamma': [0, 0.5, 2, 5],
    'min_child_weight': [0, 1, 3, 5],
}
  
grid = GridSearchCV(XGBClassifier(), param_grid, refit = True, verbose = 0, scoring='accuracy', cv = 5, n_jobs=-1)

grid.fit(X_train, y_train)

# print best parameter after tuning
print("The best parameters after tuning are: ", grid.best_params_)
  
# print how our model looks after hyper-parameter tuning
print("The best model after tuning looks like: ",grid.best_estimator_)

# Save to file in the current working directory
pkl_filename = "products/pickle_files/XGB_Grid_Tuned.pkl"
with open(pkl_filename, 'wb') as file:
  pickle.dump(grid, file)

# Load from file
with open(pkl_filename, 'rb') as file:
  pickle_model = pickle.load(file)
  score = pickle_model.score(X_test, y_test)
  print("Grid-Search Tuned XGBoost Test Set Accuracy: {0:.2f} %".format(100 * score))
  grid_predictions = pickle_model.predict(X_test)
```

```{python}
conf_matrix = confusion_matrix(y_test, grid_predictions)
disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=['Not Fraud','Fraud'], )

# save the plot

disp.plot(cmap='YlGnBu',values_format='d',ax=None)
plt.title('Test Set Confusion Matrix for Grid-Search Tuned XGBoost', fontsize=15, color='#336699',loc='center')
plt.savefig('products/plots/confusion_matrix_XGB_Grid_Tuned.png')
```

# Classification with RandomSearchCV (8 Points)

## RandomForestClassifier()

```{python}
# defining parameter range
param_search = params = {
    'n_estimators': [1, 2, 4, 8, 16, 32, 64, 100, 200],
    'max_depth': [3, 5, 7],
    'criterion': ["gini", "entropy"]
    'min_samples_leaf': [0.1, 0.2, 0.3, 0.4, 0.5],
    'max_features': list(range(1,train.shape[1])),
}
  
search = RandomizedSearchCV(RandomForestClassifier(), param_search, refit = True, verbose = 0, n_iter=25, scoring='accuracy', cv = 5, n_jobs=-1)

search.fit(X_train, y_train)

# print best parameter after tuning
print("The best parameters after tuning are: ", search.best_params_)
  
# print how our model looks after hyper-parameter tuning
print("The best model after tuning looks like: ",search.best_estimator_)

# Save to file in the current working directory
pkl_filename = "RF_Rand_Tuned.pkl"
with open(pkl_filename, 'wb') as file:
  pickle.dump(search, file)

# Load from file
with open(pkl_filename, 'rb') as file:
  pickle_model = pickle.load(file)
  score = pickle_model.score(X_test, y_test)
  print("Grid-Search Tuned Random Forest Test Set Accuracy: {0:.2f} %".format(100 * score))
  search_predictions = pickle_model.predict(X_test)
```

```{python}
conf_matrix = confusion_matrix(y_test, search_predictions)
disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=['Not Fraud','Fraud'], )

# save the plot

disp.plot(cmap='YlGnBu',values_format='d',ax=None)
plt.title('Test Set Confusion Matrix for Grid-Search Tuned Random Forest', fontsize=15, color='#336699',loc='center')
plt.savefig('confusion_matrix_RF_Rand_Tuned.png')
```

## BaggingClassifier()

```{python}
# defining parameter range
param_search = params = {
    'n_estimators': [1, 2, 4, 8, 16, 32, 64, 100, 200],
    'max_samples': [0.1*X_train.shape[0], 0.3*X_train.shape[0], 0.5*X_train.shape[0], 0.7*X_train.shape[0], X_train.shape[0]],
    'max_features': [0.1*X_train.shape[1], 0.3*X_train.shape[1], 0.5*X_train.shape[1], 0.7*X_train.shape[1], X_train.shape[1]],
    'bootstrap_features': [True, False],
    'bootstrap': [True, False],
    'oob_score': [True, False],
    'warm_start': [True, False],
}
  
search = RandomizedSearchCV(BaggingClassifier(), param_search, refit = True, verbose = 0, n_iter=25, scoring='accuracy', cv = 5, n_jobs=-1)

search.fit(X_train, y_train)

# print best parameter after tuning
print("The best parameters after tuning are: ", search.best_params_)
  
# print how our model looks after hyper-parameter tuning
print("The best model after tuning looks like: ",search.best_estimator_)

# Save to file in the current working directory
pkl_filename = "Bagging_Rand_Tuned.pkl"
with open(pkl_filename, 'wb') as file:
  pickle.dump(search, file)

# Load from file
with open(pkl_filename, 'rb') as file:
  pickle_model = pickle.load(file)
  score = pickle_model.score(X_test, y_test)
  print("Grid-Search Tuned Bagging Test Set Accuracy: {0:.2f} %".format(100 * score))
  search_predictions = pickle_model.predict(X_test)
```

```{python}
conf_matrix = confusion_matrix(y_test, search_predictions)
disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=['Not Fraud','Fraud'], )

# save the plot

disp.plot(cmap='YlGnBu',values_format='d',ax=None)
plt.title('Test Set Confusion Matrix for Grid-Search Tuned Bagging', fontsize=15, color='#336699',loc='center')
plt.savefig('confusion_matrix_Bagging_Rand_Tuned.png')
```

## XGBClassifier()

```{python}
# defining parameter range
param_search = params = {
    'max_depth': [2, 3, 5, 10, 20],
    'max_leaves': [0, 1, 2, 3, 4, 5],
    'learning_rate': [1, 0.1, 0.01, 0.001, 0.0001],
    'n_estimators': [1, 2, 4, 8, 16, 32, 64, 100, 200],
    'booster': ['gbtree', 'gblinear', 'dart'],
    'gamma': [0, 0.5, 2, 5],
    'min_child_weight': [0, 1, 3, 5],
}
  
search = RandomizedSearchCV(XGBClassifier(), param_search, refit = True, verbose = 0, n_iter=25, scoring='accuracy', cv = 5, n_jobs=-1)

search.fit(X_train, y_train)

# print best parameter after tuning
print("The best parameters after tuning are: ", search.best_params_)
  
# print how our model looks after hyper-parameter tuning
print("The best model after tuning looks like: ",search.best_estimator_)

# Save to file in the current working directory
pkl_filename = "XGB_Rand_Tuned.pkl"
with open(pkl_filename, 'wb') as file:
  pickle.dump(search, file)

# Load from file
with open(pkl_filename, 'rb') as file:
  pickle_model = pickle.load(file)
  score = pickle_model.score(X_test, y_test)
  print("Grid-Search Tuned XGBoost Test Set Accuracy: {0:.2f} %".format(100 * score))
  search_predictions = pickle_model.predict(X_test)
```

```{python}
conf_matrix = confusion_matrix(y_test, search_predictions)
disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=['Not Fraud','Fraud'], )

# save the plot

disp.plot(cmap='YlGnBu',values_format='d',ax=None)
plt.title('Test Set Confusion Matrix for Grid-Search Tuned XGBoost', fontsize=15, color='#336699',loc='center')
plt.savefig('confusion_matrix_XGB_Rand_Tuned.png')
```

# Comparison and Analysis (5 Points)

Compare the results from Q2, Q3, and Q4. Describe the best hyperparameters for all three experiments.

